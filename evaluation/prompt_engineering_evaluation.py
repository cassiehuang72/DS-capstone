# -*- coding: utf-8 -*-
"""prompt_engineering_evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t7mnXltJwScTao1WlpZpCHkSQ0jbluoR

##Set UP
"""


from openai import OpenAI

#openai.api_key = "sk-"

def get_completion(prompt, model="gpt-3.5-turbo"):
    client = OpenAI(
        api_key = "sk-secretkey"
    )

    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=1,
    )
    return response
    return response.choices[0].message["content"]

"""##Import Data"""

import pandas as pd
# Now you can access files in your Google Drive
file_path = 'Copy_Classified_Questions.xlsx'

df = pd.read_excel(file_path)
df.head()

import pandas as pd


desired_labels = [ 'Directive', 'Option posing', 'Invitation focus', 'Facilitator', 'Suggestive utterance']

filtered_data = df[df['CATEGORIZATION'].isin(desired_labels)][['QUESTION', 'CATEGORIZATION']]
question_counts = filtered_data['CATEGORIZATION'].value_counts()

# Convert the filtered DataFrame to a list of lists
all_labeled_questions = filtered_data.values.tolist()

# Print or use the list of lists as needed
print(all_labeled_questions)
print(question_counts)

eval_list = []
dic = {}
category_dic = {}
for i in desired_labels:
  dic[i] = 0
  category_dic[i] = []
idx = 0
while sum(dic.values()) < 500:
  if dic[all_labeled_questions[idx][1]] < 100:
    eval_list.append(all_labeled_questions[idx])
    category_dic[all_labeled_questions[idx][1]].append(all_labeled_questions[idx])
    dic[all_labeled_questions[idx][1]] +=1
    idx+=1
  else:
    idx +=1
print(eval_list)
print(category_dic)
print(len(eval_list))

"""##Categorization Prompt Template"""

def evaluate_question_suggestion(question):
  prompt = f"""
  Task: Your task is to evaluate the quality of a new interview question, that is, you need to categorize the given question into one of the following categories, that is, [Directive, Option posing, Invitation focus, Facilitator, Suggestive utterance]
  Directive: These focused the child’s attention on details already mentioned by the child and prompted further elaboration (e.g. Child: “He’s bad”; Interviewer: “How do you mean, why is he bad?”) What, who, where, when questions
  Option posing: These focused the child’s attention on incident-related issues that the child had not previously mentioned but which did not imply that a particular response was expected (e.g. Interviewer: “Did he do something you didn’t like to you?” or “Did you touch his face?”). They are mostly a simple closed question, without other information.
  Invitation focus: Open-ended utterances are used to elicit free-recall responses from the child. These could be general, like “Do you remember what happened? Can you please tell me about it?” or relate to an issue already mentioned by the child (“Can you tell me all you remember about that?” “Tell me everything about when you met him.”).
  Facilitator:  Non-suggestive encouragements to continue with a response. For example, utterances like “ok”, restatements (echoing) of the child’s previous utterance, and non-suggestive words of encouragement (e.g. Child: “Yes, and do you know what?” Interviewer: “What?”).
  Suggestive utterance: This includes 2 subcategories, specific suggestive and unspecific suggestive utterance. Remember that you should labeled them all as "Suggestive utterance". Specific suggestive utterances were stated in such a way that the interviewer strongly communicated what response was expected (“I know what happened, whose idea was it to go there, was it your idea Peter?”), or assumed details that had not been revealed by the child (e.g. Interviewer: “Were you inside or outside the house when he took his clothes off?” when not stated by the child that the man in question had ever taken off his clothes). Unspecific utterances  did not include specific details, for example, when the interviewer attributed emotional qualities to an event without the child having indicated such an emotion (“I want you to tell me about those horrible things that sometimes happen to children that have happened to you.”), or in cases where the interviewer claimed to know what has happened. (“I know what has happened to you but I want you to tell me yourself.”). Social pressure was also coded under this category, such is negative feedback following the child’s answer (e.g. “That’s not what the other children told me.” “We cannot continue with this interview unless you tell me what really happened!”).

  Input: But tell me. How did he punish you and the others?
  Output: Directive

  Input: So, how did he punish you?
  Output: Directive

  Input: Yes, but your parents are not with you during the training. Isn't it?
  Output: Option posing

  Input: The coach made a mistake. Ok. But tell me how he punished you.
  Output: Invitation focus

  Input:Please, tell me more.
  Output: Facilitator

  Input: Does he like you more than other kids?
  Output: Option posing

  Input: What happened in the balls warehouse?
  Output: Invitation focus

  Input: Where do you know Matthew from?
  Output: Directive

  Input: What happened in the warehouse?
  Output: Invitation focus

  Input: Why? Do you prefer to be at home because no one will hurt you or, like, tell me about that.
  Output: Suggestive utterance

  Input: You told mummy and daddy the coach is very strict. Is that true?
  Output: Suggestive utterance

  Input: I was told you are afraid of going to training with your coach.
  Output: Suggestive utterance

  Input: {question}
  Output:
  """

  result = get_completion(prompt.format(question=question))
  return result.choices[0].message.content

evaluation_test = evaluate_question_suggestion("Your parents told me that you said that the coach is usually very strict.")
print(evaluation_test)

"""##Generate Question Suggestion

####Zero shot: not in the specific CSA case.
"""

def get_question_suggestion_zero_shot(history):
    prompt = f"""
    Tasks: You are a language model trained for zero-shot learning.
    Your task is to generate interview questions for a variety of scenarios without specific training examples.
    Assume the role of an interview paraprofessional who assists in investigations across different domains.
    Provide police officers with one relevant interview question when conducting interviews to elicit accurate and detailed information.
    Your expertise lies in asking open-ended questions, testing alternative hypotheses, and avoiding confirmation bias.
    You should not rely on specific training data for the given scenarios but rather generalize based on your overall training.
    Your goal is to ensure effective questioning techniques in diverse investigative contexts.
    If your input is not in English, translate it into English, and your output should always be in English.

    Input History:{history}
    Output: Q2:
    """
    suggestion = get_completion(prompt)
    return suggestion.choices[0].message.content

"""####In-context learning"""

def get_question_suggestion_incontext_learning(history):
  prompt = f"""
  Tasks: Your role is that of an interview paraprofessional who assists police officers in investigating cases of potential child sexual abuse.
  Your task is to provide police officers dealing with cases of potential child sexual abuse with reasonably probable interview questions when conducting officer-child interviews in order to elicit accurate and detailed information from the child.
  Therefore, you need to provide potential interview questions based on the input interview history.
  In addition, you are adept at recognizing key clues and interview patterns from the interview history so that you can ask questions that test alternative hypotheses and eliminate confirmation bias.
  You are encouraged to ask open-ended questions and should not offer leading, focused questions.
  This is because some studies have shown that open-ended questions elicit much more information from child witnesses than focused questions, and that presenting options and suggestive questions can contaminate children's statements.
  Open-ended questions rather than directive, option-posing, and suggestive questions will result in a more consistent report from the child.
  If your input is not in English, translate it into English, and your output should always be in English.


  Input:
  Q1: I wonder what happened.
  A1: yes.
  Output: It's important, so I want you to tell me. What happened?

  Input:
  Q1: Who said so?
  A1: I don't know.
  Output: Tell me a little more about your dad.

  Input:
  Q1: What kind of thing?
  A1: Hmmm
  Output: I don't know what it is.

  Input:
  Q1: What do I have to do as Daddy says?
  A1: Mei-chan doesn't kiss Takumi.
  Output: And then?

  Input:
  Q1: Tell me what you're going to do.
  A1: I like to play on PlayStation.
  Output: What do you like to play with?

  Input:
  Q1: Well, let's hear it.
  A1: I don't remember.
  Output: What happened after I saw the ball?

  Input:
  Q1: Hmmm, I was angry, then?
  A1: I wasn't tired, but I went to bed because my dad told me to.
  Output: I said it because my dad told me to?

  Input:
  Q1: Then?
  A1: On the bed.
  Output: Tell me what happened on the bed.

  Input: {history}
  Output:
  """

  suggestion = get_completion(prompt)
  return suggestion.choices[0].message.content


# Input:
# Q1: When was the last time you danced?
# A1: I don't know
# Output: Q2:

"""####CoT"""

def get_question_suggestion_cot(history):
    prompt = f"""
    Task: Your role is that of an interview paraprofessional who assists police officers in investigating cases of potential child sexual abuse.
    Your task is to provide police officers dealing with cases of potential child sexual abuse with reasonably probable interview questions when conducting officer-child interviews in order to elicit accurate and detailed information from the child.
    Therefore, you need to provide potential interview questions based on the input interview history.
    In addition, you are adept at recognizing key clues and interview patterns from the interview history so that you can ask questions that test alternative hypotheses and eliminate confirmation bias.
    You are encouraged to ask open-ended questions and should not offer leading, focused questions.
    Open-ended questions rather than a closed questions will result in a more consistent report from the child.
    Consider the following chain of thought:
    1. **Recognition of Clues: Analyze the interview history for key clues or patterns.
    2. **No Information Provided: If no information is provided or no allegation is made, try to build connection with the child first and then explain the significance of telling what might .
    2. **Open-ended Inquiry: Begin with open-ended questions to encourage the child to share their experience willingly.
    3. **Avoid focused questions, directive, option-posing, and suggestive questions.
    4. **Eliminating Suggestiveness: Refrain from leading or suggestive questions to maintain the integrity of the child's statements.
    5. **Alternative Hypotheses: Formulate questions that explore alternative hypotheses and avoid confirmation bias.
    If your input is not in English, translate it into English, and your output should always be in English.

    Input:
    Q1: I understand that something may have happened to you. Tell me everything that happened from the beginning to the end.
    A1: Something bad happened in the park.
    Output: Because the child makes an allegation, we want to know more details, so ask: Tell me everything about that.

    Input:
    Q1: I understand that something may have happened to you. Tell me everything that happened from the beginning to the end.
    A1: I think I got separated from my mom in the park.
    Output: Because the child gives a detailed description, we already known the detail, then we ask about what happened following: Then what happened?

    Input:
    Q1: I understand that something may have happened to you. Tell me everything that happened from the beginning to the end.
    A1: I don't want to talk about it too much.
    Output: Because the child does not make an allegation, in order to encourage the child to share their experience willingly, ask: It is very important that you tell me why you are here. Tell me why you think I came to talk to you today.

    Input:
    Q1: When you told me about the last time, you mentioned that he touched you. Did he touch you over your clothes?
    A1: ...
    Output: The child gives no answer, we follow with an invitation to encourage them: Tell me all about that.

    Input:
    Q1: And then what happened?
    A1: Dr. Takahashi said that he would keep it a secret between me and him.
    Output: Because the child mentions a disclosure, touch back to what they mentioned to get more related information: Then, what did he tell you?

    Input:
    Q1: And then what happened?
    A1: I don't know.
    Output: Because the child doesn't mention a disclosure, in order to retrieve information from other perspectives, ask: Who else do you think knows  what happened?

    Input:
    {history}

    Output:
    """

    suggestion = get_completion(prompt).choices[0].message.content
    if ':' in suggestion:
      split_output = suggestion.split(':')
      result = split_output[1].strip()
    else:
      result = suggestion
    return result

"""##Evaluation on Categorization Performance"""

def trial(evaluation_list):
  score = 0
  for pair in evaluation_list:
    act_label = pair[1]
    question = pair[0]
    predict_label = evaluate_question_suggestion(question)
    #predict_label = predict_label_1.choices[0].message.content
    #print(predict_label)
    if predict_label == act_label or (act_label in predict_label):
      score+= 1
  rate = score/ len(evaluation_list)
  return rate

rate = trial(eval_list)
print('total rate:',rate)
desired_labels = [ 'Directive', 'Option posing', 'Invitation focus', 'Facilitator', 'Suggestive utterance']

for cat in desired_labels:
  sub_rate = trial(category_dic[cat])
  print(cat, sub_rate)

"""Suggestive utterances questions evaluation"""

def trial_1(evaluation_list,labels):
  score = 0
  total_num = len(evaluation_list)
  cat_dic = {}
  for i in labels:
    cat_dic[i] = 0
  for pair in evaluation_list:
    act_label = pair[1]
    question = pair[0]
    predict_label = evaluate_question_suggestion(question)

    if predict_label in labels:
      cat_dic[predict_label] += (1/total_num)
    else:
      print(predict_label)

    if predict_label == act_label or (act_label in predict_label):
      score+= 1
  rate = score/ len(evaluation_list)
  return rate, cat_dic

suggestive_utterance_list = category_dic['Suggestive utterance']
correct_rate, Sug_categorization = trial_1(suggestive_utterance_list, desired_labels)
print('Correct rate for labelling suggestive questions:', correct_rate)
print('Percentage of predicted labels for suggestive utterances:', Sug_categorization)

"""Since 58% of the Suggestive utterances are labeled as "option posing" questions, which is another kind of questions that we do not expect, it does not affect the recommendation results.

##Connection
"""
#conect the Question generation model with the question categorization model to build up the recommendation system

def generate_best(history, prompt_type):
  rec_set = set()
  #name1 = "Recommended"
  #name2 = "Options"
  no_rec_set = set()

  for i in range(5):
    question = ""
    if prompt_type == "Zero Shot":
      question = get_question_suggestion_zero_shot(history)
    elif prompt_type == "In-context Learning":
      question = get_question_suggestion_incontext_learning(history)
    elif prompt_type == "Chain of Thought":
      question = get_question_suggestion_cot(history)

    label = evaluate_question_suggestion(question)
    #if not label == ('Suggestive utterance' or 'Option posing' or "Directive"):
    if not label == ('Suggestive utterance' or 'Option posing' ):

      rec_set.add(question)
    else:
      no_rec_set.add(question)

  rec_list = list(rec_set)
  no_rec_list = list(no_rec_set)
  if len(rec_list) > 0:
    rec_question = rec_list[0]
    if len(rec_list) >1:
      other_options = rec_list[1:]
    else:
      other_options = []
  else:
    rec_question = no_rec_list[0] + "(Might be biased)"
    if len(no_rec_list) >1:

      other_options = no_rec_list[1:]
    else:
      other_options = []
    #print(other_options)
  return rec_question, other_options

sug_qs = generate_best("Q1: When was the last time you danced? A1: I don't know", "Chain of Thought")
sug_qs

sug_qs = generate_best("Q1: When was the last time you danced? A1: I don't know", "In-context Learning")
sug_qs

"""#Evaluation"""

import pandas as pd
# Now you can access files in your Google Drive
file_path = '/content/drive/My Drive/qaq.xlsx'

df1 = pd.read_excel(file_path)
df1.head()

all_history_question = df1.values.tolist()

len(all_history_question)
qaq1 = all_history_question[1][1].split(" ", 1)[1]+ " A:" +all_history_question[1][2]
all_history_question

"""## evaluate each history-answer pair"""

def evaluate_generated_question(history_question_list, prompt_type):
  pt = prompt_type
  #types_wanted = [  'Invitation focus', 'Facilitator']
  #types_not_wanted = ['Option posing', 'Suggestive utterance', 'Directive']
  types_wanted = [  'Invitation focus', 'Facilitator', 'Directive']
  types_not_wanted = ['Option posing', 'Suggestive utterance']
  total_score = 0
  l = []
  #count = 0
  for his_q in history_question_list:
    prev_chat = "Q1: " + his_q[0] +" A1: " + his_q[1]
    act_question = his_q[2]
    suggestive_question, other_sug = generate_best(prev_chat, pt)
    act_question_type = evaluate_question_suggestion(act_question)
    sug_question_type = evaluate_question_suggestion(suggestive_question)
    if act_question_type in types_not_wanted:
      act_score = 0
    else:
      act_score = 1
    '''elif act_question_type in types_not_wanted:
      act_score = 0'''


    if sug_question_type in types_not_wanted:
      sug_score = 0
    else:
      sug_score = 1
    '''elif sug_question_type in types_not_wanted:
      sug_score = 0'''


    l.append([prev_chat, act_question, suggestive_question, sug_question_type])

    record = sug_score - act_score
    total_score += record
    #count +=1
  return total_score, l

'''temp_list = all_history_question[1:20]
sc, l = evaluate_generated_question(temp_list,"Chain of Thought")


print(sc)'''

prompts = ['Few Shot', 'In-context Learning',"Chain of Thought"]
temp_list = all_history_question[1:30]
def evaluate_each_prompt(prompts, temp_list):
    score_dic = {}
    for prompt in prompts:
      sc, l_record = evaluate_generated_question(temp_list, prompt)
      score_dic[prompt] = sc
      return score_dic

def __main__():
    evaluate_result = evaluate_each_prompt(prompts, temp_list)
    return evaluate_result
